{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRy4RbnW6vCEW4gvARV2mZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gafm-ai/astro_learn/blob/main/prepareData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas_ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovnYwsO_MD-7",
        "outputId": "4c8d07fa-ec65-4f40-d923-839ff2f68346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas_ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.16.0)\n",
            "Building wheels for collected packages: pandas_ta\n",
            "  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218909 sha256=5e87adedbc136e2e921803ea1f21908f898f2f87a3fbdcee0c2b2457a78920bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/00/ac/f7fa862c34b0e2ef320175100c233377b4c558944f12474cf0\n",
            "Successfully built pandas_ta\n",
            "Installing collected packages: pandas_ta\n",
            "Successfully installed pandas_ta-0.3.14b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import numpy as np\n",
        "import logging\n",
        "import pytz\n",
        "import json"
      ],
      "metadata": {
        "id": "DqUNrxokLsW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XYqWvX2LzEd",
        "outputId": "b53fa052-9dfe-4690-f72e-c8127711301a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aczCEya0LVMb"
      },
      "outputs": [],
      "source": [
        "# Configuration du logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Fonction pour vérifier le timeframe\n",
        "def verify_timeframe(timeframe):\n",
        "    valid_timeframes = ['1min', '5min', '15min', '30min', '1H', '4H', '1D']\n",
        "    if timeframe not in valid_timeframes:\n",
        "        logger.error(f\"Timeframe invalide : {timeframe}. Veuillez choisir parmi {valid_timeframes}.\")\n",
        "        raise ValueError(\"Timeframe invalide.\")\n",
        "    logger.info(f\"Timeframe sélectionné : {timeframe}\")\n",
        "\n",
        "# Fonction pour charger et préparer les données\n",
        "def load_and_prepare_data(file_path, timeframe):\n",
        "    # Charger les données\n",
        "    df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
        "\n",
        "    # Renommer les colonnes\n",
        "    #df.rename(columns={'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}, inplace=True)\n",
        "\n",
        "    # Utiliser 'timestamp' comme index\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    # Ordonner les données par date croissante\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # S'assurer que l'index est en timezone UTC\n",
        "    if df.index.tz is None:\n",
        "        # Si ce n'est pas le cas, localiser en UTC\n",
        "        df.index = df.index.tz_localize(pytz.utc)\n",
        "    else:\n",
        "        # Si l'index a déjà un fuseau horaire, convertir en UTC\n",
        "        df.index = df.index.tz_convert(pytz.utc)\n",
        "\n",
        "    print(df.head())\n",
        "\n",
        "    print(f\"Données chargées : {df.shape[0]} lignes.\")\n",
        "    return df\n",
        "\n",
        "# Fonction pour gérer les gaps temporels et combler les données manquantes\n",
        "def handle_time_gaps(df, timeframe):\n",
        "    # Définir les jours ouvrables (lundi=0, ..., vendredi=4)\n",
        "    business_days = [0, 1, 2, 3, 4]\n",
        "\n",
        "    # Créer un index complet sans week-ends\n",
        "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=timeframe, tz=pytz.utc)\n",
        "    full_index = full_index[full_index.dayofweek.isin(business_days)]\n",
        "\n",
        "    # Reindexer le DataFrame\n",
        "    df = df.reindex(full_index)\n",
        "\n",
        "    # Combler les données manquantes\n",
        "    df['close'] = df['close'].ffill()\n",
        "    df['open'] = df['open'].fillna(df['close'])\n",
        "    df['high'] = df['high'].fillna(df['close'])\n",
        "    df['low'] = df['low'].fillna(df['close'])\n",
        "    df['volume'] = df['volume'].fillna(0)\n",
        "\n",
        "    print(\"Données manquantes comblées.\")\n",
        "    return df\n",
        "\n",
        "# Fonction pour calculer les indicateurs\n",
        "def calculate_indicators(df):\n",
        "    # Calculer les chandeliers Heikin-Ashi\n",
        "    ha = pd.DataFrame(index=df.index)\n",
        "    ha['HA_Close'] = (df['open'] + df['high'] + df['low'] + df['close']) / 4\n",
        "    ha_open = ((df['open'].shift(1) + df['close'].shift(1)) / 2)\n",
        "    ha['HA_Open'] = ha_open.fillna(df['open'])\n",
        "    ha['HA_High'] = pd.concat([ha['HA_Open'], ha['HA_Close'], df['high']], axis=1).max(axis=1)\n",
        "    ha['HA_Low'] = pd.concat([ha['HA_Open'], ha['HA_Close'], df['low']], axis=1).min(axis=1)\n",
        "\n",
        "\n",
        "    print(\"Chandeliers Heikin-Ashi calculés.\")\n",
        "\n",
        "    # Calculer les SMA et leur lissage\n",
        "    ha['SMA_60'] = ta.sma(ha['HA_Close'], length=60)\n",
        "    ha['SMA_60_Smoothed'] = ta.sma(ha['SMA_60'], length=5)\n",
        "    ha['SMA_20'] = ta.sma(ha['HA_Close'], length=20)\n",
        "    ha['SMA_20_Smoothed'] = ta.sma(ha['SMA_20'], length=5)\n",
        "    ha['SMA_5'] = ta.sma(ha['HA_Close'], length=5)\n",
        "    ha['SMA_5_Smoothed'] = ta.sma(ha['SMA_5'], length=5)\n",
        "\n",
        "    # Calculer les autres indicateurs\n",
        "\n",
        "    ha[['TSI_13_25_13', 'TSIs_13_25_13']] = ta.tsi(ha['HA_Close']).values\n",
        "    ha[['VTXP_14', 'VTXM_14']] = ta.vortex(ha['HA_High'], ha['HA_Low'], ha['HA_Close'], length=14).values\n",
        "    ha['MFI_14'] = ta.mfi(ha['HA_High'], ha['HA_Low'], ha['HA_Close'], df['volume'])\n",
        "    ha['AO_5_34'] = ta.ao(ha['HA_High'], ha['HA_Low'])\n",
        "\n",
        "    ha['VWAP'] = ta.vwap(ha['HA_High'], ha['HA_Low'], ha['HA_Close'], df['volume'])\n",
        "\n",
        "    # Ajouter des patterns avec talib\n",
        "    #ha['Doji'] = talib.CDLDOJI(ha['HA_Open'], ha['HA_High'], ha['HA_Low'], ha['HA_Close'])\n",
        "\n",
        "    print(\"Indicateurs calculés.\")\n",
        "    return ha\n",
        "\n",
        "# Fonction pour détecter les croisements SMA\n",
        "def detect_sma_crossings(ha):\n",
        "    # Créer des colonnes booléennes pour les croisements\n",
        "    ha['Cross_SMA_60'] = ((ha['SMA_60'] > ha['SMA_60_Smoothed']) & (ha['SMA_60'].shift(1) <= ha['SMA_60_Smoothed'].shift(1))) | \\\n",
        "                         ((ha['SMA_60'] < ha['SMA_60_Smoothed']) & (ha['SMA_60'].shift(1) >= ha['SMA_60_Smoothed'].shift(1)))\n",
        "\n",
        "    ha['Cross_SMA_20'] = ((ha['SMA_20'] > ha['SMA_20_Smoothed']) & (ha['SMA_20'].shift(1) <= ha['SMA_20_Smoothed'].shift(1))) | \\\n",
        "                         ((ha['SMA_20'] < ha['SMA_20_Smoothed']) & (ha['SMA_20'].shift(1) >= ha['SMA_20_Smoothed'].shift(1)))\n",
        "\n",
        "    ha['Cross_SMA_5'] = ((ha['SMA_5'] > ha['SMA_5_Smoothed']) & (ha['SMA_5'].shift(1) <= ha['SMA_5_Smoothed'].shift(1))) | \\\n",
        "                        ((ha['SMA_5'] < ha['SMA_5_Smoothed']) & (ha['SMA_5'].shift(1) >= ha['SMA_5_Smoothed'].shift(1)))\n",
        "\n",
        "    ha['Cross_SMA_5_20'] = ((ha['SMA_5'] > ha['SMA_20']) & (ha['SMA_5'].shift(1) <= ha['SMA_20'].shift(1))) | \\\n",
        "                    ((ha['SMA_5'] < ha['SMA_20']) & (ha['SMA_5'].shift(1) >= ha['SMA_20'].shift(1)))\n",
        "\n",
        "    ha['Cross_SMA_5_60'] = ((ha['SMA_5'] > ha['SMA_60']) & (ha['SMA_5'].shift(1) <= ha['SMA_60'].shift(1))) | \\\n",
        "                    ((ha['SMA_5'] < ha['SMA_60']) & (ha['SMA_5'].shift(1) >= ha['SMA_60'].shift(1)))\n",
        "\n",
        "    ha['Cross_SMA_20_60'] = ((ha['SMA_20'] > ha['SMA_60']) & (ha['SMA_20'].shift(1) <= ha['SMA_60'].shift(1))) | \\\n",
        "                    ((ha['SMA_20'] < ha['SMA_60']) & (ha['SMA_20'].shift(1) >= ha['SMA_60'].shift(1)))\n",
        "\n",
        "    return ha\n",
        "\n",
        "# Fonction pour calculer les labels\n",
        "def calculate_labels(df, pip_value=0.0001):\n",
        "    # Fonction interne pour calculer les labels pour une direction\n",
        "    def _calculate_directional_labels(df, pip_targets, periods, direction='buy'):\n",
        "        labels = pd.DataFrame(index=df.index)\n",
        "        for pip_target, period in zip(pip_targets, periods):\n",
        "            if direction == 'buy':\n",
        "                future_high = df['high'].shift(-1).rolling(window=period).max()\n",
        "                target_price = df['close'] + (pip_target * pip_value)\n",
        "                label = (future_high >= target_price).astype(int)\n",
        "            elif direction == 'sell':\n",
        "                future_low = df['low'].shift(-1).rolling(window=period).min()\n",
        "                target_price = df['close'] - (pip_target * pip_value)\n",
        "                label = (future_low <= target_price).astype(int)\n",
        "            else:\n",
        "                raise ValueError(\"Direction doit être 'buy' ou 'sell'\")\n",
        "            label_name = f\"{direction}_{pip_target}pip_{period}bars\"\n",
        "            labels[label_name] = label\n",
        "        return labels\n",
        "\n",
        "    # Définir les pip targets et les périodes pour les achats\n",
        "    pip_targets_buy = [1.5, 1.5, 2.5, 2.5, 5.5, 9]\n",
        "    periods_buy = [2, 5, 5, 10, 10, 10]\n",
        "\n",
        "    # Calculer les labels pour les achats\n",
        "    labels_buy = _calculate_directional_labels(df, pip_targets_buy, periods_buy, direction='buy')\n",
        "    print(\"Labels d'achat calculés.\")\n",
        "\n",
        "    # Définir les pip targets et les périodes pour les ventes\n",
        "    pip_targets_sell = [1.5, 1.5, 2.5, 2.5, 5.5, 9]\n",
        "    periods_sell = [2, 5, 5, 10, 10, 10]\n",
        "\n",
        "    # Calculer les labels pour les ventes\n",
        "    labels_sell = _calculate_directional_labels(df, pip_targets_sell, periods_sell, direction='sell')\n",
        "    print(\"Labels de vente calculés.\")\n",
        "\n",
        "    # Combiner les labels\n",
        "    labels = pd.concat([labels_buy, labels_sell], axis=1)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction principale pour exécuter le pipeline de traitement\n",
        "def main(file_path, timeframe='1min', sequence_length=32):\n",
        "    verify_timeframe(timeframe)\n",
        "    df = load_and_prepare_data(file_path, timeframe)\n",
        "    df = handle_time_gaps(df, timeframe)\n",
        "    ha_df = calculate_indicators(df)\n",
        "\n",
        "    # Joindre les données originales pour le calcul des labels\n",
        "    data = ha_df.join(df[['open', 'high', 'low', 'close', 'volume']], how='left')\n",
        "\n",
        "    # Supprimer les lignes avec des valeurs manquantes\n",
        "    data.dropna(inplace=True)\n",
        "    print(f\"Données après suppression des valeurs manquantes : {data.shape[0]} lignes.\")\n",
        "\n",
        "    # Ajouter une colonne 'date' pour identifier le jour de trading\n",
        "    data['date'] = data.index.date\n",
        "\n",
        "    # Ajouter les colonnes 'hour', 'minute', 'second' pour le temps\n",
        "    data['hour'] = data.index.hour\n",
        "    data['minute'] = data.index.minute\n",
        "    #data['second'] = data.index.second\n",
        "    print(data.columns)\n",
        "    # Calculer les labels\n",
        "    labels = calculate_labels(data)\n",
        "    data = data.join(labels)\n",
        "    data.dropna(inplace=True)\n",
        "    print(f\"Données après ajout des labels : {data.shape[0]} lignes.\")\n",
        "\n",
        "    # Détecter les croisements SMA\n",
        "    data = detect_sma_crossings(data)\n",
        "\n",
        "    # Début de la création des séquences\n",
        "    print(\"Début de la création des séquences.\")\n",
        "\n",
        "    # Préparer les features et labels\n",
        "    feature_columns = ['HA_Close', 'HA_Open', 'HA_High', 'HA_Low', 'SMA_60', 'SMA_60_Smoothed',\n",
        "       'SMA_20', 'SMA_20_Smoothed', 'SMA_5', 'SMA_5_Smoothed', 'TSI_13_25_13',\n",
        "       'TSIs_13_25_13', 'VTXP_14', 'VTXM_14', 'MFI_14', 'AO_5_34', 'VWAP',\n",
        "       'open', 'high', 'low', 'close', 'volume', 'hour', 'minute']\n",
        "    label_columns = labels.columns.tolist()\n",
        "\n",
        "    # Créer les séquences (fenêtres) de longueur 32\n",
        "    X = []\n",
        "    y = []\n",
        "    json_data = []\n",
        "\n",
        "    total_sequences = len(data) - sequence_length + 1\n",
        "    print(f\"Nombre total de séquences possibles : {total_sequences}\")\n",
        "\n",
        "    for i in range(total_sequences):\n",
        "        sequence_data = data.iloc[i:i+sequence_length]\n",
        "        last_row = sequence_data.iloc[-1]\n",
        "\n",
        "        # Obtenir le timestamp de la dernière observation\n",
        "        timestamp = last_row.name\n",
        "\n",
        "        # S'assurer que timestamp est un objet Timestamp\n",
        "        if not isinstance(timestamp, pd.Timestamp):\n",
        "            timestamp = pd.to_datetime(timestamp)\n",
        "\n",
        "        # Vérifier que la dernière observation est dans l'intervalle 7h-19h\n",
        "        if timestamp.time() >= pd.to_datetime('07:00').time() and timestamp.time() <= pd.to_datetime('19:00').time():\n",
        "            # Vérifier que toutes les observations de la séquence sont du même jour\n",
        "            if sequence_data['date'].nunique() == 1:\n",
        "                # Vérifier si un croisement SMA se produit à la dernière étape de la séquence\n",
        "                if last_row['Cross_SMA_60'] or last_row['Cross_SMA_20'] or last_row['Cross_SMA_5'] or last_row['Cross_SMA_5_60'] or last_row['Cross_SMA_5_60'] or last_row['Cross_SMA_20_60']:\n",
        "                    X_sequence = sequence_data[feature_columns]\n",
        "                    y_sequence = sequence_data[label_columns].iloc[-1]\n",
        "                    X.append(X_sequence.values)\n",
        "                    y.append(y_sequence.values)\n",
        "\n",
        "                    # Préparer les features avec le format souhaité\n",
        "                    features_dict = {}\n",
        "                    for col in feature_columns:\n",
        "                        features_dict[col] = X_sequence[col].tolist()\n",
        "\n",
        "                    # Préparer les labels avec les noms de colonnes\n",
        "                    label_dict = y_sequence.to_dict()\n",
        "\n",
        "                    # Préparer l'entrée pour le JSON\n",
        "                    entry = {\n",
        "                        'features': features_dict,\n",
        "                        'labels': label_dict\n",
        "                    }\n",
        "                    json_data.append(entry)\n",
        "        else:\n",
        "            # Afficher une information si nécessaire\n",
        "            if i % 100000 == 0:\n",
        "                print(f\"Séquence ignorée car hors de l'intervalle 7h-19h à l'index {timestamp}\")\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    print(f\"Séquences finales créées : X.shape = {X.shape}, y.shape = {y.shape}\")\n",
        "\n",
        "    # Enregistrer les données au format .npy\n",
        "    np.save('/content/drive/MyDrive/FLEURY/Finances/new32_1/X.npy', X)\n",
        "    np.save('/content/drive/MyDrive/FLEURY/Finances/new32_1/y.npy', y)\n",
        "    print(\"Données enregistrées au format .npy.\")\n",
        "\n",
        "    # Enregistrer les données au format JSON pour le LLM\n",
        "    with open('/content/drive/MyDrive/FLEURY/Finances/new32_1/data.json', 'w') as json_file:\n",
        "        json.dump(json_data, json_file)\n",
        "    print(\"Données enregistrées au format JSON avec les features structurées.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main('/content/drive/MyDrive/FLEURY/Finances/data/OANDA_EUR_USD_1_20120101_20221231_price.csv', timeframe='5min', sequence_length=12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryOQxtDWLcOp",
        "outputId": "4e501ca9-5dd2-44d5-8917-51ee63ba29f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              open     high      low    close  volume\n",
            "timestamp                                                            \n",
            "2012-01-01 17:57:00+00:00  1.29525  1.29525  1.29425  1.29425       2\n",
            "2012-01-01 17:58:00+00:00  1.29416  1.29416  1.29416  1.29416       1\n",
            "2012-01-01 18:00:00+00:00  1.29409  1.29409  1.29409  1.29409       1\n",
            "2012-01-01 18:01:00+00:00  1.29393  1.29403  1.29388  1.29403       4\n",
            "2012-01-01 18:02:00+00:00  1.29408  1.29416  1.29263  1.29332      23\n",
            "Données chargées : 3937808 lignes.\n",
            "Données manquantes comblées.\n",
            "Chandeliers Heikin-Ashi calculés.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-56-21be7b5c9fb8>:87: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ 1.29459     0.          0.         ... 31.04024667 12.84369\n",
            " 78.127885  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  ha['MFI_14'] = ta.mfi(ha['HA_High'], ha['HA_Low'], ha['HA_Close'], df['volume'])\n",
            "<ipython-input-56-21be7b5c9fb8>:87: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[  1.29460667   0.          25.88298333 ...  41.740205    66.35581\n",
            " 120.91922833]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  ha['MFI_14'] = ta.mfi(ha['HA_High'], ha['HA_Low'], ha['HA_Close'], df['volume'])\n",
            "<ipython-input-56-21be7b5c9fb8>:90: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
            "  ha['VWAP'] = ta.vwap(ha['HA_High'], ha['HA_Low'], ha['HA_Close'], df['volume'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicateurs calculés.\n",
            "Données après suppression des valeurs manquantes : 809844 lignes.\n",
            "Index(['HA_Close', 'HA_Open', 'HA_High', 'HA_Low', 'SMA_60', 'SMA_60_Smoothed',\n",
            "       'SMA_20', 'SMA_20_Smoothed', 'SMA_5', 'SMA_5_Smoothed', 'TSI_13_25_13',\n",
            "       'TSIs_13_25_13', 'VTXP_14', 'VTXM_14', 'MFI_14', 'AO_5_34', 'VWAP',\n",
            "       'open', 'high', 'low', 'close', 'volume', 'date', 'hour', 'minute'],\n",
            "      dtype='object')\n",
            "Labels d'achat calculés.\n",
            "Labels de vente calculés.\n",
            "Données après ajout des labels : 809844 lignes.\n",
            "Début de la création des séquences.\n",
            "Nombre total de séquences possibles : 809833\n",
            "Séquence ignorée car hors de l'intervalle 7h-19h à l'index 2012-01-02 06:12:00+00:00\n",
            "Séquence ignorée car hors de l'intervalle 7h-19h à l'index 2014-09-19 02:27:00+00:00\n",
            "Séquence ignorée car hors de l'intervalle 7h-19h à l'index 2016-02-01 06:42:00+00:00\n",
            "Séquence ignorée car hors de l'intervalle 7h-19h à l'index 2017-06-09 00:42:00+00:00\n",
            "Séquence ignorée car hors de l'intervalle 7h-19h à l'index 2018-10-18 00:42:00+00:00\n",
            "Séquences finales créées : X.shape = (98595, 12, 24), y.shape = (98595, 12)\n",
            "Données enregistrées au format .npy.\n",
            "Données enregistrées au format JSON avec les features structurées.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3XMwazpMhxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}